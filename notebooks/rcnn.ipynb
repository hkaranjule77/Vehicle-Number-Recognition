{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rural-national",
   "metadata": {},
   "source": [
    "# RCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-robinson",
   "metadata": {},
   "source": [
    "**Reference:** https://www.youtube.com/watch?v=IcLEJB2pY2Y&t=2055s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "powerful-chemical",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False    # for autocompletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "analyzed-pathology",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations\n",
    "import glob\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "published-station",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants: configurations\n",
    "\n",
    "DATA_DIR = os.path.join('..', 'datasets', 'gen-plate-dataset')\n",
    "BATCH_SIZE = 8\n",
    "IMAGE_WIDTH = 230\n",
    "IMAGE_HEIGHT = 50\n",
    "NUM_WORKERS = 2\n",
    "DEVICE = 'cpu'    # cpu / cuda\n",
    "EPOCHS = 4       # in actual initialized 200 but trained till 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "musical-international",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "pretty-passport",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# dataset creations\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "class ClassificationDataset:\n",
    "    def __init__(self, img_paths, targets, resize = None):\n",
    "        self.img_paths = img_paths\n",
    "        self.targets = targets\n",
    "        self.resize = resize\n",
    "        self.aug = albumentations.Compose(\n",
    "            [albumentations.Normalize(always_apply=True)]\n",
    "        )\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    def __getitem__(self, item_index):\n",
    "        img = Image.open(self.img_paths[item_index])\n",
    "        targets = self.targets[item_index]\n",
    "        \n",
    "        if self.resize is not None:\n",
    "            img = img.resize((self.resize[0], self.resize[1]), resample= Image.BILINEAR)\n",
    "        \n",
    "        img = np.array(img)\n",
    "        augmented = self.aug(image = img)\n",
    "        img = augmented['image']\n",
    "        img = np.transpose(img, (2, 1, 0)).astype(np.float32)\n",
    "        return {\n",
    "            'imgs': torch.tensor(img, dtype=torch.float),\n",
    "            'targets': torch.tensor(targets, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-squad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "exempt-surname",
   "metadata": {},
   "source": [
    "# Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "appropriate-spanking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# engine\n",
    "\n",
    "def train_fn(model, data_loader, optimizer):\n",
    "    model.train()\n",
    "    fin_loss = 0\n",
    "    tk = tqdm(data_loader, total=len(data_loader))\n",
    "    for data in tk:\n",
    "        for k, v in data.items():\n",
    "            data[k] = v.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        _, loss = model(**data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        fin_loss += loss.item()\n",
    "        \n",
    "    return fin_loss / len(data_loader)\n",
    "\n",
    "def eval_fn(model, data_loader, optimizer):\n",
    "    model.eval()\n",
    "    fin_loss = 0\n",
    "    fin_preds = []\n",
    "    with torch.no_grad():\n",
    "        tk = tqdm(data_loader, total=len(data_loader))\n",
    "        for data in tk:\n",
    "            for k, v in data.items():\n",
    "                data[k] = v.to(DEVICE)\n",
    "            batch_preds, loss = model(**data)\n",
    "            \n",
    "            fin_loss += loss.item()\n",
    "            fin_preds.append(batch_preds)\n",
    "        \n",
    "        fin_preds = torch.cat(fin_preds, dim=0)\n",
    "        return fin_preds, fin_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-brick",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "alternate-cornell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "class PlateModel(nn.Module):\n",
    "    def __init__(self, num_chars):\n",
    "        super(PlateModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 128, kernel_size=(3,3), padding=(1,1))\n",
    "        self.max_pool_1 = nn.MaxPool2d(kernel_size=(2,2))\n",
    "        self.conv2 = nn.Conv2d(128, 64, kernel_size=(3,3), padding=(1,1))\n",
    "        self.max_pool_2 = nn.MaxPool2d(kernel_size=(2,2))\n",
    "        \n",
    "        self.linear1 = nn.Linear(768, 512)\n",
    "        self.linear2 = nn.Linear(512, 64)\n",
    "        self.drop = nn.Dropout(0.2)   # doesn't change size\n",
    "        \n",
    "        self.gru = nn.GRU(64, 32, bidirectional=True, num_layers=2, dropout=0.25)\n",
    "        self.output = nn.Linear(64, num_chars + 1)\n",
    "        \n",
    "    def forward(self, imgs, targets=None):\n",
    "        bs, c, w, h = imgs.size()\n",
    "        # print(bs, c, w, h)    # for debugging\n",
    "        x = F.relu(self.conv1(imgs))\n",
    "        # print('Conv1', x.size())\n",
    "        x = self.max_pool_1(x)\n",
    "        # print('MaxPool', x.size())\n",
    "        x = F.relu(self.conv2(x))\n",
    "        # print('Conv2', x.size())\n",
    "        x = self.max_pool_2(x) # 1, 64, 212, 64\n",
    "        # print('MaxPool', x.size())\n",
    "        \n",
    "        # to brind width first but in our case it's properly arranged\n",
    "        # x = x.permute(0, 3, 1, 2) # 1, 75, 64, 18\n",
    "        x = x.view(bs, x.size(2), -1)\n",
    "        # print('View', x.size())\n",
    "        \n",
    "        x = self.linear1(x)\n",
    "        # print('Linear1', x.size())\n",
    "        x = self.linear2(x)\n",
    "        x = self.drop(x)\n",
    "        # print('Linear2', x.size())\n",
    "        \n",
    "        x, _ = self.gru(x)\n",
    "        # print('GRU', x.size())\n",
    "        \n",
    "        x = self.output(x)\n",
    "        # print('output', x.size())\n",
    "        \n",
    "        x = x.permute(1, 0, 2)\n",
    "        if targets is not None:\n",
    "            # CTC\n",
    "            log_softmax_values = F.log_softmax(x, 2)\n",
    "            input_lengths = torch.full(\n",
    "                size=(bs,), fill_value = log_softmax_values.size(0), dtype=torch.int32\n",
    "            )\n",
    "            # print('input lengths', input_lengths)\n",
    "            target_lengths = torch.full(\n",
    "                size=(bs,), fill_value = log_softmax_values.size(1), dtype=torch.int32\n",
    "            )\n",
    "            # print('target lengths', target_lengths)\n",
    "            loss = nn.CTCLoss(blank=0)(\n",
    "                log_softmax_values, targets, input_lengths, target_lengths\n",
    "            )\n",
    "            return x, loss\n",
    "        \n",
    "        return x, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "corresponding-bedroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for debugging model\n",
    "'''\n",
    "cm = PlateModel(19)\n",
    "img = torch.rand(5, 3, IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "targets = torch.randint(1, 6, (5, 5))\n",
    "x, loss = cm(img, targets)\n",
    "\n",
    "del(cm)\n",
    "''';\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-discharge",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "national-belize",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate img & target list\n",
    "\n",
    "def get_img_label():\n",
    "    '''Returns tuple of img filename list and target_label list.'''\n",
    "    img_files = glob.glob(os.path.join(DATA_DIR, '*.png'))\n",
    "    targets_orig = [x.split('/')[-1][ : -4] for x in img_files]\n",
    "    return img_files, targets_orig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "conceptual-numbers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target preprocessing\n",
    "\n",
    "def get_target_list(target_orig):\n",
    "    targets = [[c for c in x] for x in targets_orig]\n",
    "    targets_flat = [c for clist in targets for c in clist]\n",
    "    return targets, targets_flat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acoustic-hanging",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target encoding\n",
    "\n",
    "def encode_labels(targets, targets_flat):\n",
    "    lbl_enc = preprocessing.LabelEncoder()\n",
    "    lbl_enc.fit(targets_flat)\n",
    "    targets_enc = [lbl_enc.transform(x) for x in targets]\n",
    "    targets_enc = np.array(targets_enc) + 1\n",
    "    return lbl_enc, targets_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "specified-certificate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode\n",
    "\n",
    "def decode_predictions(preds, encoder, collapse_repeated=True):\n",
    "    ''' Decodes CTC String to normal string'''\n",
    "    preds = preds.permute(1, 0, 2)\n",
    "    preds = torch.softmax(preds, 2)\n",
    "    preds = torch.argmax(preds, 2)\n",
    "    preds = preds.detach().cpu().numpy() # change cpu to cuda if training on gpu\n",
    "    cap_preds = []\n",
    "    for j in range(preds.shape[0]):\n",
    "        temp = ''\n",
    "        prev_char = None\n",
    "        for k in preds[j]:\n",
    "            k = k-1\n",
    "            # k = -1 mean a empty value\n",
    "            if (k == -1) or (collapse_repeated and k==prev_char):\n",
    "                continue\n",
    "            else:\n",
    "                # print(encoder.inverse_transform([k]), k)\n",
    "                temp += encoder.inverse_transform([k])[0]\n",
    "            prev_char = k\n",
    "        tp = \"\" + temp\n",
    "        # print(preds[j], targets[j], tp)\n",
    "        cap_preds.append(tp)\n",
    "    return cap_preds\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "nervous-sailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks if all characters of targets are in predictions\n",
    "\n",
    "def char_accuracy(preds, targets):\n",
    "    sum_accuracy = 0\n",
    "    total_preds = 0\n",
    "    for index in range(len(preds)):\n",
    "        correct_char = 0\n",
    "        for char in preds[index]:\n",
    "            if char in targets[index]:\n",
    "                correct_char += 1\n",
    "        accuracy = correct_char / len(preds[index])\n",
    "        sum_accuracy += accuracy\n",
    "        total_preds += 1\n",
    "    return sum_accuracy / total_preds\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "essential-rotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks if all predictions are same as labels\n",
    "\n",
    "def label_accuracy(preds, targets):\n",
    "    correct_labels = 0\n",
    "    for index in range(len(preds)):\n",
    "        if preds[index] == targets[index]:\n",
    "            correct_labels += 1\n",
    "    return correct_labels / len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "elegant-liabilities",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing & encoding\n",
    "\n",
    "img_files, targets_orig = get_img_label()\n",
    "\n",
    "targets, targets_flat = get_target_list(targets_orig)\n",
    "lbl_enc, targets_enc = encode_labels(targets, targets_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "expanded-shaft",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data splitting\n",
    "\n",
    "( train_imgs, test_imgs,\n",
    " train_targets,\n",
    " test_targets,\n",
    " train_orig_targets,\n",
    " test_orig_targets \n",
    ") = model_selection.train_test_split(\n",
    "    img_files, targets_enc, targets_orig, test_size=0.1, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "necessary-directive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataset & loader\n",
    "\n",
    "train_dataset = ClassificationDataset(\n",
    "                    img_paths=train_imgs, \n",
    "                    targets=train_targets,\n",
    "                    resize=(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "                )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                    train_dataset,\n",
    "                    batch_size = BATCH_SIZE,\n",
    "                    num_workers = NUM_WORKERS,\n",
    "                    shuffle = True\n",
    "                )\n",
    "\n",
    "# for debugging\n",
    "\n",
    "# npimg = train_dataset[0]['imgs'].numpy()\n",
    "# plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "particular-ideal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset & loader\n",
    "\n",
    "test_dataset = ClassificationDataset(\n",
    "                    img_paths = test_imgs,\n",
    "                    targets = test_targets,\n",
    "                    resize = (IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "                )\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                test_dataset,\n",
    "                batch_size = BATCH_SIZE,\n",
    "                num_workers = NUM_WORKERS,\n",
    "                shuffle = False\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "reflected-constant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, optimizer & schedular\n",
    "\n",
    "model = PlateModel(num_chars=len(lbl_enc.classes_))\n",
    "model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "schedular = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, factor=0.8, patience=5, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "constant-groove",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual training\n",
    "\n",
    "def run_training(model, train_loader, test_loader, optimizer, schedular, lbl_enc):\n",
    "    loss = {'train': [], 'valid': []}\n",
    "    accuracy = {'character': [], 'label': []}\n",
    "    \n",
    "    epoch_count = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = train_fn(model, train_loader, optimizer)\n",
    "        valid_preds, valid_loss = eval_fn(model, test_loader, optimizer)\n",
    "        \n",
    "        valid_cap_preds = []\n",
    "        for vp in tqdm(valid_preds, total=len(valid_preds)):\n",
    "            current_preds = decode_predictions(valid_preds, lbl_enc)\n",
    "            valid_cap_preds.extend(current_preds)\n",
    "            \n",
    "        char_acc = char_accuracy(test_orig_targets, valid_cap_preds)\n",
    "        label_acc = label_accuracy(test_orig_targets, valid_cap_preds)\n",
    "            \n",
    "        # calculate accuracy of model and log it   \n",
    "        pprint(list(zip(test_orig_targets[6:15], valid_cap_preds))[6:15])\n",
    "        print(f\"Epoch:{epoch}, train_loss:{train_loss}, valid_loss={valid_loss}\")\n",
    "        print(f\"char_accuracy:{char_acc}, label_accuracy:{label_acc}\")\n",
    "        \n",
    "        loss['train'].append(train_loss)\n",
    "        loss['valid'].append(valid_loss)\n",
    "        accuracy['character'].append(char_acc)\n",
    "        accuracy['label'].append(label_acc)\n",
    "        \n",
    "        epoch_count += 1\n",
    "        \n",
    "    return loss, accuracy, epoch_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-permit",
   "metadata": {},
   "source": [
    "# Start Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "protecting-translation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/945 [00:00<?, ?it/s]/home/hkaranjule77/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  Variable._execution_engine.run_backward(\n",
      "100%|██████████| 945/945 [03:32<00:00,  4.45it/s]\n",
      "100%|██████████| 105/105 [00:10<00:00, 10.04it/s]\n",
      "100%|██████████| 5985/5985 [03:31<00:00, 28.26it/s]\n",
      "  0%|          | 0/945 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('MH 02WE 2573', ''), ('MH 01GY 8049', ''), ('MH 61FJ 2862', '')]\n",
      "Epoch:0, train_loss:3.373100360991463, valid_loss=3.0898458889552525\n",
      "char_accuracy:0.0, label_accuracy:0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 945/945 [03:30<00:00,  4.49it/s]\n",
      "100%|██████████| 105/105 [00:10<00:00,  9.84it/s]\n",
      "100%|██████████| 5985/5985 [21:48<00:00,  4.57it/s]\n",
      "  0%|          | 0/945 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('MH 02WE 2573',\n",
      "  'MH01AWMH01AWMH0AWMH01AWMH01AWMH01AWMH01AWMH0AWMH01AWMH0AWMH01AWMH01AWMH01AWMH01AWMH01AWMH0AWMH0AWMH0AWMH01AWMH01AWMH01AWMH01AWMH01AWMH01AWMH01AWMH01AWMH01AWMH01AWMH0AWMH0AWMH0AWMH01AWMH0AWMH21AWMH0AWMH0AWMH0AWMH01AWMH01AWMH01AWMH01AWMH0AWMH0AWMH0AWMH01AWMH01AWMH01AWMH01AWMH01AWMH0AWMH01AWMH01AWMH01AWMH01AWMH01AWMH01AWMH01AWMH01AWMH21AWMH01AWMH01AWMH01AWMH01AWMH01AWMH0AWMH01AWMH01AWMH01AWMH01AWMH01AWMH0AWMH01AWMH01AWMH01AWMH01AWMH01AWMH0AWMH21AWMH0AWMH01AWMH01AWMH0AWMH01AWMH01AWMH0AWMH01AWMH01AWMH01AWMH01AWMH01AWMH01AWMH0AWMH01AWMH01AWMH21AWMH01AWMH01AWMH01AWMH01AWMH0AWMH01AWMH0AWMH21AWMH01AWMH21AW'),\n",
      " ('MH 01GY 8049',\n",
      "  'MH0AYMH01AYMH01AYMH0AYMH31AYMH0AYMH01AYMH01AYMH0AYMH0AWMH31AYMH01AYMH0AYMH01AWMH01AYMH0AWMH01AYMH01AWMH0AYMH01AYMH01AYMH01AYMH01AYMH01AYMH01AYMH0AYMH0AYMH01AYMH0AYMH0AYMH01AYMH1AYMH01AYMH31AYMH0AYMH01AWMH01AYMH01AYMH0AYMH01AYMH01AYMH01AYMH01AYMH01AYMH0AYMH0AYMH01AYMH31AYMH31AYMH01AYMH01AYMH01AYMH01AYMH01AYMH0AYMH01AWMH0AYMH01AYMH31AYMH0AYMH01AYMH0AYMH01AYMH01AYMH01AYMH0AYMH01AYMH01AYMH31AYMH0AYMH01AYMH0AYMH1AYMH0AYMH01AWMH1AYMH01AYMH31AYMH0AYMH0AYMH01AYMH0AYMH01AYMH01AYMH01AYMH0AYMH1AYMH01AWMH0AYMH31AYMH0AYMH01AYMH0AYMH01AYMH31AYMH01AYMH01AYMH01AWMH0AWMH0AYMH03AYMH01AWMH31AYMH01AYMH31AY'),\n",
      " ('MH 61FJ 2862',\n",
      "  'MH01MOMH01MOMH0MOMH0MOMH01MOMH01MOMH01MOMH01MOMH01MOMH0MOMH01MOMH0MOMH01MOMH0MOMH0MOMH0MOMH01MOMH0MOMH01MOMH01MOMH01MOMH01MOMH01MOMH01MOMH01MOMH01MOMH01MOMH01MOMH01MOMH01MOMH01MOMH0MOMH0MOMH01MOMH01MOMH0MOMH0MOMH01MOMH01MOMH01MOMH01MOMH01MOMH0MOMH01MOMH01MOMH01MOMH0MOMH0MOMH01MOMH01MOMH01MOMH01MOMH01MOMH01MOMH01MOMH0MOMH01MOMH0MOMH01MOMH01MOMH01MOMH01MOMH01MOMH01MOMH01MOMH01MOMH01MOMH0MOMH01MOMH01MOMH0MOMH01MOMH01MOMH0MOMH01MOMH01MOMH01MOMH01MOMH01MOMH0MOMH01MOMH01MOMH01MOMH0MOMH01MOMH01MOMH0MOMH0MOMH0MOMH01MOMH0MOMH01MOMH01MOMH0MOMH01MOMH01MOMH01MOMH0MOMH0MOMH01MOMH01MOMH01MOMH01MOMH01MOMH01MO')]\n",
      "Epoch:1, train_loss:2.195117903891064, valid_loss=1.6721657060441517\n",
      "char_accuracy:0.33164682539682516, label_accuracy:0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 945/945 [03:35<00:00,  4.38it/s]\n",
      "100%|██████████| 105/105 [00:10<00:00, 10.23it/s]\n",
      "100%|██████████| 5985/5985 [20:08<00:00,  4.95it/s]\n",
      "  0%|          | 0/945 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('MH 02WE 2573',\n",
      "  'MH30RMH41RMH01RMH4RMH50RMH4RMH45RMH0RMH30RMH01RMH1RMH10RMH23RMH1RMH05RMH03RMH01RMH0RMH21RMH53RMH54RMH23RMH20RMH5RMH17RMH10RMH15RMH05RMH0RMH01RMH03RMH47RMH0RMH34RMH03RMH03RMH0RMH07RMH50RMH19RMH5RMH03RMH0RMH03RMH1RMH57RMH41RMH53RMH50RMH0RMH52RMH20RMH1RMH45RMH50RMH10RMH4RMH5RMH13RMH07RMH17RMH45RMH0RMH51RMH01RMH50RMH51RMH4RMH3RMH21RMH01RMH30RMH53RMH05RMH52RMH23RMH02RMH10RMH01RMH41RMH53RMH0RMH04RMH45RMH0RMH07RMH35RMH54RMH40RMH3RMH47RMH02RMH01RMH05RMH3RMH15RMH07RMH1RMH54RMH01RMH37RMH03RMH45RMH15RMH51R'),\n",
      " ('MH 01GY 8049',\n",
      "  'MH01RMH3RMH51RMH03RMH5RMH03RMH58RMH23RMH03RMH02RMH15RBMH1RBMH0RMH17RBMH50RMH0RMH20RMH17RBMH03RMH37RMH42RMH32RMH51RMH17RBMH63RMH0RMH0RMH5RMH01RMH03RMH14RMH41RBMH51RMH14RBMH0RMH1RMH12RMH30RMH03RMH37RMH37RMH34RMH40RBMH37RMH01RMH0RMH5RMH17RBMH45RBMH17RBMH17RMH30RMH5RMH0RMH01RMH15RBMH03RMH37RMH17RBMH01RMH4RBMH02RMH47RBMH41RBMH07RMH0RMH53RMH07RMH5RMH02RMH43RBMH0RMH51RMH03RMH10RBMH34RMH47RBMH57RMH01RMH03RMH14RBMH01RMH20RBMH35RMH29RMH0RMH1RBMH12RBMH01RMH41RBMH0RMH47RMH01RMH30RMH14RBMH51RMH34RMH13RBMH03RMH03RMH20RMH25RBMH31RMH5RMH4RB'),\n",
      " ('MH 61FJ 2862',\n",
      "  'MH50HNMH10HNMH03HNMH03HNMH07HNMH53HNMH12HNMH30HNMH12HNMH01HNMH04HNMH03HNMH10HNMH03HNMH01HNMH03HNMH51HNMH03HNMH50HNMH51HNMH57HNMH13HNMH57HNMH14HNMH15HNMH53HNMH17HNMH1HNMH45HNMH13HNMH53HNMH0HNMH03HNMH14HNMH05HNMH01HNMH03HNMH27HNMH53HNMH30HNMH54HNMH57HNMH01HNMH13HNMH17HNMH5HNMH03HNMH03HNMH51HNMH3HNMH43HNMH41HNMH51HNMH07HNMH51HNMH01HNMH1HNMH03HNMH50HNMH50HNMH10HNMH17HNMH30HNMH12HNMH31HNMH47HNMH50HNMH0HNMH50HNMH10HNMH0HNMH51HNMH42HNMH03HNMH17HNMH41HNMH30HNMH5HNMH30HNMH03HNMH54HNMH51HNMH50HNMH03HNMH50HNMH53HNMH0HNMH03HNMH0HNMH35HNMH0HNMH37HNMH35HNMH01HNMH10HNMH59HNMH10HNMH01HNMH01HNMH07HNMH47HNMH14HNMH59HNMH15HNMH54HN')]\n",
      "Epoch:2, train_loss:1.5772816917883656, valid_loss=1.4127411910465786\n",
      "char_accuracy:0.5918650793650795, label_accuracy:0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 945/945 [03:41<00:00,  4.27it/s]\n",
      "100%|██████████| 105/105 [00:10<00:00,  9.61it/s]\n",
      "100%|██████████| 5985/5985 [26:58<00:00,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('MH 02WE 2573',\n",
      "  'MH 38K MH 61K MH 01K MH 64K MH 60K MH 4K MH 46K MH 0K MH 30K MH 01K MH 12K '\n",
      "  'MH 20K MH 23K MH 1K MH 05K MH 02K MH 01K MH 0K MH 21K MH 63K MH 64K MH 23K '\n",
      "  'MH 20K MH 5K MH 29K MH 10K MH 16K MH 06K MH 0K MH 01K MH 03K MH 67K MH 0K '\n",
      "  'MH 36K MH 02K MH 03K MH 0K MH 07K MH 58K MH 19K MH 56K MH 02K MH 0K MH 02K '\n",
      "  'MH 1K MH 67K MH 42K MH 53K MH 58K MH 0K MH 52K MH 28K MH 1K MH 45K MH 58K '\n",
      "  'MH 28K MH 4K MH 5K MH 13K MH 09K MH 19K MH 46K MH 08K MH 51K MH 01K MH 50K '\n",
      "  'MH 51K MH 45K MH 3K MH 2K MH 01K MH 38K MH 53K MH 05K MH 62K MH 23K MH 02K '\n",
      "  'MH 28K MH 01K MH 61K MH 53K MH 0K MH 05K MH 46K MH 0K MH 07K MH 36K MH 54K '\n",
      "  'MH 48K MH 3K MH 47K MH 02K MH 04K MH 06K MH 3K MH 15K MH 07K MH 21K MH 56K '\n",
      "  'MH 02K MH 37K MH 02K MH 46K MH 26K MH 51K '),\n",
      " ('MH 01GY 8049',\n",
      "  'MH 01K MH 3K MH 51K MH 02K MH 56K MH 03K MH 50K MH 23K MH 02K MH 02K MH 16K '\n",
      "  'MH 12K MH 0K MH 19K MH 60K MH 0K MH 20K MH 17K MH 02K MH 37K MH 62K MH 34K '\n",
      "  'MH 61K MH 19K MH 63K MH 0K MH 0K MH 5K MH 01K MH 03K MH 24K MH 42K MH 52K '\n",
      "  'MH 16K MH 0K MH 2K MH 2K MH 30K MH 02K MH 39K MH 37K MH 34K MH 40K MH 39K '\n",
      "  'MH 01K MH 0K MH 65K MH 19K MH 46K MH 17K MH 27K MH 38K MH 56K MH 08K MH 01K '\n",
      "  'MH 26K MH 03K MH 37K MH 27K MH 01K MH 4K MH 02K MH 49K MH 42K MH 09K MH 0K '\n",
      "  'MH 63K MH 09K MH 5K MH 02K MH 43K MH 0K MH 51K MH 03K MH 18K MH 34K MH 49K '\n",
      "  'MH 67K MH 01K MH 03K MH 24K MH 01K MH 28K MH 36K MH 29K MH 0K MH 21K MH 12K '\n",
      "  'MH 01K MH 41K MH 0K MH 47K MH 01K MH 38K MH 14K MH 51K MH 36K MH 13K MH 02K '\n",
      "  'MH 03K MH 20K MH 26K MH 31K MH 5K MH 4K '),\n",
      " ('MH 61FJ 2862',\n",
      "  'MH 60N MH 20N MH 03N MH 02N MH 09N MH 63N MH 2N MH 38N MH 2N MH 01N MH 04N '\n",
      "  'MH 02N MH 10N MH 03N MH 02N MH 03N MH 51N MH 02N MH 50N MH 62N MH 57N MH '\n",
      "  '13N MH 57N MH 24N MH 26N MH 53N MH 17N MH 1N MH 45N MH 13N MH 63N MH 0N MH '\n",
      "  '02N MH 24N MH 06N MH 01N MH 03N MH 27N MH 53N MH 38N MH 56N MH 57N MH 01N '\n",
      "  'MH 13N MH 29N MH 56N MH 02N MH 02N MH 51N MH 3N MH 43N MH 64N MH 51N MH 07N '\n",
      "  'MH 61N MH 01N MH 2N MH 02N MH 58N MH 58N MH 20N MH 27N MH 38N MH 12N MH 31N '\n",
      "  'MH 49N MH 50N MH 0N MH 50N MH 18N MH 0N MH 52N MH 62N MH 03N MH 17N MH 42N '\n",
      "  'MH 30N MH 5N MH 38N MH 02N MH 54N MH 51N MH 60N MH 02N MH 60N MH 53N MH 0N '\n",
      "  'MH 02N MH 0N MH 36N MH 0N MH 37N MH 36N MH 01N MH 18N MH 59N MH 10N MH 01N '\n",
      "  'MH 01N MH 07N MH 49N MH 24N MH 59N MH 15N MH 54N ')]\n",
      "Epoch:3, train_loss:1.1131875965330336, valid_loss=0.9072595653079805\n",
      "char_accuracy:0.8521825396825417, label_accuracy:0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy, epoch_count = run_training(model, train_loader, test_loader, optimizer, schedular, lbl_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passing-justice",
   "metadata": {},
   "source": [
    "## ENDGAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-spare",
   "metadata": {},
   "source": [
    "### Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bridal-printer",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = os.path.join('..','train')\n",
    "\n",
    "try:\n",
    "    os.mkdir(TRAIN_DIR)\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "rocky-consolidation",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINER_DIR = os.path.join(TRAIN_DIR,'harshad')    # change this\n",
    "\n",
    "try:\n",
    "    os.mkdir(TRAINER_DIR)\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "annoying-density",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this for every training\n",
    "# or it will overwrite your previous data\n",
    "\n",
    "VER_DIR = os.path.join(TRAINER_DIR, 'text_recognition ver-1.0')\n",
    "\n",
    "try:\n",
    "    os.mkdir(VER_DIR)\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "usual-vehicle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving weights & optimizer\n",
    "\n",
    "WT_PATH = os.path.join(VER_DIR, 'weights.pth')\n",
    "torch.save(model.state_dict(), WT_PATH)\n",
    "\n",
    "OPTIM_PATH = os.path.join(VER_DIR, 'optimizer.pth')\n",
    "torch.save(model.state_dict(), OPTIM_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-vietnamese",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "allied-locking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving hyperparametes\n",
    "\n",
    "HYP_PATH = os.path.join(VER_DIR, 'hyperparam.json')\n",
    "\n",
    "hyper_dict = dict()\n",
    "hyper_dict[\"INITIALIED EPOCH\"] = EPOCHS\n",
    "hyper_dict[\"ACTUAL EPOCH\"] = epoch_count\n",
    "hyper_dict[\"MODEL\"] = str(model.parameters)\n",
    "hyper_dict[\"LOSS\"] = dict()\n",
    "hyper_dict[\"LOSS\"][\"train\"] = [float(train_loss) for train_loss in loss['train']]\n",
    "hyper_dict[\"LOSS\"][\"valid\"] = [float(valid_loss) for valid_loss in loss['valid']]\n",
    "hyper_dict[\"ACCURACY\"] = dict()\n",
    "hyper_dict[\"ACCURACY\"][\"char\"] = [float(train_loss) for train_loss in accuracy['character']]\n",
    "hyper_dict[\"ACCURACY\"][\"label\"] = [float(valid_loss) for valid_loss in accuracy['label']]\n",
    "\n",
    "\n",
    "hyp_file = open(HYP_PATH, \"w\")\n",
    "hyperparam_json = json.dump(hyper_dict, fp=hyp_file)\n",
    "hyp_file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-vertex",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
