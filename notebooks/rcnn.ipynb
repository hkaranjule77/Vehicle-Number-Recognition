{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rural-national",
   "metadata": {},
   "source": [
    "# RCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-robinson",
   "metadata": {},
   "source": [
    "**Reference:** https://www.youtube.com/watch?v=IcLEJB2pY2Y&t=2055s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "powerful-chemical",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False    # for autocompletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "analyzed-pathology",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations\n",
    "import glob\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "published-station",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants: configurations\n",
    "\n",
    "DATA_DIR = os.path.join('..', 'datasets', 'gen-plate-dataset')\n",
    "BATCH_SIZE = 8\n",
    "IMAGE_WIDTH = 230\n",
    "IMAGE_HEIGHT = 50\n",
    "NUM_WORKERS = 2\n",
    "DEVICE = 'cpu'    # cpu / cuda\n",
    "EPOCHS = 1       # in actual initialized 200 but trained till 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "musical-international",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "pretty-passport",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# dataset creations\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "class ClassificationDataset:\n",
    "    def __init__(self, img_paths, targets, resize = None):\n",
    "        self.img_paths = img_paths\n",
    "        self.targets = targets\n",
    "        self.resize = resize\n",
    "        self.aug = albumentations.Compose(\n",
    "            [albumentations.Normalize(always_apply=True)]\n",
    "        )\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    def __getitem__(self, item_index):\n",
    "        img = Image.open(self.img_paths[item_index])\n",
    "        targets = self.targets[item_index]\n",
    "        \n",
    "        if self.resize is not None:\n",
    "            img = img.resize((self.resize[0], self.resize[1]), resample= Image.BILINEAR)\n",
    "        \n",
    "        img = np.array(img)\n",
    "        augmented = self.aug(image = img)\n",
    "        img = augmented['image']\n",
    "        img = np.transpose(img, (2, 1, 0)).astype(np.float32)\n",
    "        return {\n",
    "            'imgs': torch.tensor(img, dtype=torch.float),\n",
    "            'targets': torch.tensor(targets, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-squad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "exempt-surname",
   "metadata": {},
   "source": [
    "# Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "appropriate-spanking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# engine\n",
    "\n",
    "def train_fn(model, data_loader, optimizer):\n",
    "    model.train()\n",
    "    fin_loss = 0\n",
    "    tk = tqdm(data_loader, total=len(data_loader))\n",
    "    for data in tk:\n",
    "        for k, v in data.items():\n",
    "            data[k] = v.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        _, loss = model(**data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        fin_loss += loss.item()\n",
    "        \n",
    "    return fin_loss / len(data_loader)\n",
    "\n",
    "def eval_fn(model, data_loader, optimizer):\n",
    "    model.eval()\n",
    "    fin_loss = 0\n",
    "    fin_preds = []\n",
    "    with torch.no_grad():\n",
    "        tk = tqdm(data_loader, total=len(data_loader))\n",
    "        for data in tk:\n",
    "            for k, v in data.items():\n",
    "                data[k] = v.to(DEVICE)\n",
    "            batch_preds, loss = model(**data)\n",
    "            \n",
    "            fin_loss += loss.item()\n",
    "            fin_preds.append(batch_preds)\n",
    "        \n",
    "        fin_preds = torch.cat(fin_preds, dim=0)\n",
    "        return fin_preds, fin_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-brick",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "alternate-cornell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "class PlateModel(nn.Module):\n",
    "    def __init__(self, num_chars):\n",
    "        super(PlateModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 128, kernel_size=(3,3), padding=(1,1))\n",
    "        self.max_pool_1 = nn.MaxPool2d(kernel_size=(2,2))\n",
    "        self.conv2 = nn.Conv2d(128, 64, kernel_size=(3,3), padding=(1,1))\n",
    "        self.max_pool_2 = nn.MaxPool2d(kernel_size=(2,2))\n",
    "        \n",
    "        self.linear1 = nn.Linear(768, 512)\n",
    "        self.linear2 = nn.Linear(512, 64)\n",
    "        self.drop = nn.Dropout(0.2)   # doesn't change size\n",
    "        \n",
    "        self.gru = nn.GRU(64, 32, bidirectional=True, num_layers=2, dropout=0.25)\n",
    "        self.output = nn.Linear(64, num_chars + 1)\n",
    "        \n",
    "    def forward(self, imgs, targets=None):\n",
    "        bs, c, w, h = imgs.size()\n",
    "        # print(bs, c, w, h)    # for debugging\n",
    "        x = F.relu(self.conv1(imgs))\n",
    "        # print('Conv1', x.size())\n",
    "        x = self.max_pool_1(x)\n",
    "        # print('MaxPool', x.size())\n",
    "        x = F.relu(self.conv2(x))\n",
    "        # print('Conv2', x.size())\n",
    "        x = self.max_pool_2(x) # 1, 64, 212, 64\n",
    "        # print('MaxPool', x.size())\n",
    "        \n",
    "        # to brind width first but in our case it's properly arranged\n",
    "        # x = x.permute(0, 3, 1, 2) # 1, 75, 64, 18\n",
    "        x = x.view(bs, x.size(2), -1)\n",
    "        # print('View', x.size())\n",
    "        \n",
    "        x = self.linear1(x)\n",
    "        # print('Linear1', x.size())\n",
    "        x = self.linear2(x)\n",
    "        x = self.drop(x)\n",
    "        # print('Linear2', x.size())\n",
    "        \n",
    "        x, _ = self.gru(x)\n",
    "        # print('GRU', x.size())\n",
    "        \n",
    "        x = self.output(x)\n",
    "        # print('output', x.size())\n",
    "        \n",
    "        x = x.permute(1, 0, 2)\n",
    "        if targets is not None:\n",
    "            # CTC\n",
    "            log_softmax_values = F.log_softmax(x, 2)\n",
    "            input_lengths = torch.full(\n",
    "                size=(bs,), fill_value = log_softmax_values.size(0), dtype=torch.int32\n",
    "            )\n",
    "            # print('input lengths', input_lengths)\n",
    "            target_lengths = torch.full(\n",
    "                size=(bs,), fill_value = log_softmax_values.size(1), dtype=torch.int32\n",
    "            )\n",
    "            # print('target lengths', target_lengths)\n",
    "            loss = nn.CTCLoss(blank=0)(\n",
    "                log_softmax_values, targets, input_lengths, target_lengths\n",
    "            )\n",
    "            return x, loss\n",
    "        \n",
    "        return x, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "corresponding-bedroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for debugging model\n",
    "\n",
    "cm = PlateModel(19)\n",
    "img = torch.rand(5, 3, IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "targets = torch.randint(1, 6, (5, 5))\n",
    "x, loss = cm(img, targets)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "helpful-drain",
   "metadata": {},
   "outputs": [],
   "source": [
    "del(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-discharge",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "national-belize",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate img & target list\n",
    "\n",
    "def get_img_label():\n",
    "    '''Returns tuple of img filename list and target_label list.'''\n",
    "    img_files = glob.glob(os.path.join(DATA_DIR, '*.png'))\n",
    "    targets_orig = [x.split('/')[-1][ : -4] for x in img_files]\n",
    "    return img_files, targets_orig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "conceptual-numbers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target preprocessing\n",
    "\n",
    "def get_target_list(target_orig):\n",
    "    targets = [[c for c in x] for x in targets_orig]\n",
    "    targets_flat = [c for clist in targets for c in clist]\n",
    "    return targets, targets_flat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "acoustic-hanging",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target encoding\n",
    "\n",
    "def encode_labels(targets, targets_flat):\n",
    "    lbl_enc = preprocessing.LabelEncoder()\n",
    "    lbl_enc.fit(targets_flat)\n",
    "    targets_enc = [lbl_enc.transform(x) for x in targets]\n",
    "    targets_enc = np.array(targets_enc) + 1\n",
    "    return lbl_enc, targets_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "specified-certificate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode\n",
    "\n",
    "def decode_predictions(preds, encoder, collapse_repeated=True):\n",
    "    ''' Decodes CTC String to normal string'''\n",
    "    preds = preds.permute(1, 0, 2)\n",
    "    preds = torch.softmax(preds, 2)\n",
    "    preds = torch.argmax(preds, 2)\n",
    "    preds = preds.detach().cpu().numpy() # change cpu to cuda if training on gpu\n",
    "    cap_preds = []\n",
    "    for j in range(preds.shape[0]):\n",
    "        temp = ''\n",
    "        for k in range(preds[j].shape[0]):\n",
    "            k = k-1\n",
    "            # k = -1 mean a empty value\n",
    "            if k == -1 or collapse_repeated and preds[j, k]==preds[j, k-1]:\n",
    "                continue\n",
    "            else:\n",
    "                temp.join(encoder.inverse_transform(preds[j, k])[0])\n",
    "        tp = \"\".join(temp)\n",
    "        cap_preds.append(tp)\n",
    "    return cap_preds\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "nervous-sailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_accuracy(preds, targets):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "essential-rotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_accuracy(preds, targets):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "elegant-liabilities",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing & encoding\n",
    "\n",
    "img_files, targets_orig = get_img_label()\n",
    "\n",
    "targets, targets_flat = get_target_list(targets_orig)\n",
    "label_enc, targets_enc = encode_labels(targets, targets_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "expanded-shaft",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data splitting\n",
    "\n",
    "( train_imgs, test_imgs,\n",
    " train_targets,\n",
    " test_targets,\n",
    " train_orig_targets,\n",
    " test_orig_targets \n",
    ") = model_selection.train_test_split(\n",
    "    img_files, targets_enc, targets_orig, test_size=0.1, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "necessary-directive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataset & loader\n",
    "\n",
    "train_dataset = ClassificationDataset(\n",
    "                    img_paths=train_imgs, \n",
    "                    targets=train_targets,\n",
    "                    resize=(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "                )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                    train_dataset,\n",
    "                    batch_size = BATCH_SIZE,\n",
    "                    num_workers = NUM_WORKERS,\n",
    "                    shuffle = True\n",
    "                )\n",
    "\n",
    "# for debugging\n",
    "\n",
    "# npimg = train_dataset[0]['imgs'].numpy()\n",
    "# plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "particular-ideal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset & loader\n",
    "\n",
    "test_dataset = ClassificationDataset(\n",
    "                    img_paths = test_imgs,\n",
    "                    targets = test_targets,\n",
    "                    resize = (IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "                )\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                test_dataset,\n",
    "                batch_size = BATCH_SIZE,\n",
    "                num_workers = NUM_WORKERS,\n",
    "                shuffle = False\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "reflected-constant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, optimizer & schedular\n",
    "\n",
    "model = PlateModel(num_chars=len(lbl_enc.classes_))\n",
    "model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "schedular = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, factor=0.8, patience=5, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "constant-groove",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual training\n",
    "\n",
    "def run_training(model, train_loader, test_loader, optimizer, schedular, lbl_enc):\n",
    "    loss = {'train': [], 'valid': []}\n",
    "    \n",
    "    epoch_count = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = train_fn(model, train_loader, optimizer)\n",
    "        valid_preds, valid_loss = eval_fn(model, test_loader, optimizer)\n",
    "        \n",
    "        print('Processing output.')\n",
    "        valid_cap_preds = []\n",
    "        for vp in tqdm(valid_preds, total=len(valid_preds)):\n",
    "            current_preds = decode_predictions(valid_preds, lbl_enc)\n",
    "            valid_cap_preds.extend(current_preds)\n",
    "            \n",
    "        # calculate accuracy of model and log it   \n",
    "        \n",
    "        pprint(list(zip(test_orig_targets[6:15], valid_cap_preds))[6:15])\n",
    "        print(f\"Epoch:{epoch}, train_loss:{train_loss}, valid_loss={valid_loss}\")\n",
    "        loss['train'].append(train_loss)\n",
    "        loss['valid'].append(valid_loss)\n",
    "        \n",
    "        epoch_count += 1\n",
    "        \n",
    "    return loss, epoch_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-permit",
   "metadata": {},
   "source": [
    "# Start Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "protecting-translation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 945/945 [03:20<00:00,  4.70it/s]\n",
      "100%|██████████| 105/105 [00:10<00:00, 10.40it/s]\n",
      "  0%|          | 3/5985 [00:00<04:17, 23.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing output.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5985/5985 [03:17<00:00, 30.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Epoch:0, train_loss:3.541047059921991, valid_loss=3.081325932911464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss, epoch_count = run_training(model, train_loader, test_loader, optimizer, schedular, lbl_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passing-justice",
   "metadata": {},
   "source": [
    "## ENDGAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-spare",
   "metadata": {},
   "source": [
    "### Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "bridal-printer",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = os.path.join('..','train')\n",
    "\n",
    "try:\n",
    "    os.mkdir(TRAIN_DIR)\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "rocky-consolidation",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINER_DIR = os.path.join(TRAIN_DIR,'harshad')    # change this\n",
    "\n",
    "try:\n",
    "    os.mkdir(TRAINER_DIR)\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "annoying-density",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this for every training\n",
    "# or it will overwrite your previous data\n",
    "\n",
    "VER_DIR = os.path.join(TRAINER_DIR, 'text_recognition ver-1.0')\n",
    "\n",
    "try:\n",
    "    os.mkdir(VER_DIR)\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "usual-vehicle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving weights & optimizer\n",
    "\n",
    "WT_PATH = os.path.join(VER_DIR, 'weights.pth')\n",
    "torch.save(model.state_dict(), WT_PATH)\n",
    "\n",
    "OPTIM_PATH = os.path.join(VER_DIR, 'optimizer.pth')\n",
    "torch.save(model.state_dict(), OPTIM_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-vietnamese",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "allied-locking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving hyperparametes\n",
    "\n",
    "HYP_PATH = os.path.join(VER_DIR, 'hyperparam.json')\n",
    "\n",
    "hyper_dict = dict()\n",
    "hyper_dict[\"INITIALIED EPOCH\"] = EPOCHS\n",
    "hyper_dict[\"ACTUAL EPOCH\"] = epoch_count\n",
    "hyper_dict[\"MODEL\"] = str(model.parameters)\n",
    "hyper_dict[\"LOSS\"] = dict()\n",
    "hyper_dict[\"LOSS\"][\"train\"] = list([float(train_loss) for train_loss in loss['train']])\n",
    "hyper_dict[\"LOSS\"][\"valid\"] = [float(valid_loss) for valid_loss in loss['valid']]\n",
    "\n",
    "\n",
    "hyp_file = open(HYP_PATH, \"w\")\n",
    "hyperparam_json = json.dump(hyper_dict, fp=hyp_file)\n",
    "hyp_file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-vertex",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
