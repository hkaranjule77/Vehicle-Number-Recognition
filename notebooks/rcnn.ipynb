{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rural-national",
   "metadata": {},
   "source": [
    "# RCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-robinson",
   "metadata": {},
   "source": [
    "**Reference:** https://www.youtube.com/watch?v=IcLEJB2pY2Y&t=2055s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powerful-chemical",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False    # for autocompletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "analyzed-pathology",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations\n",
    "import glob\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "published-station",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants: configurations\n",
    "if os.path.exists('gen-plate-dataset'):\n",
    "    DATA_DIR = 'gen-plate-dataset'\n",
    "else:\n",
    "    DATA_DIR = os.path.join('..', 'datasets', 'gen-plate-dataset')\n",
    "BATCH_SIZE = 4\n",
    "IMAGE_WIDTH = 230\n",
    "IMAGE_HEIGHT = 50\n",
    "NUM_WORKERS = 2\n",
    "DEVICE = 'cpu'    # cpu / cuda\n",
    "EPOCHS = 50       # in actual initialized 200 but trained till 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "frequent-ending",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.sys.platform=='linux' and os.path.exists('../input/indmhnumberplate/gen-plate-dataset'):\n",
    "    DATA_DIR = '../input/indmhnumberplate/gen-plate-dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "automotive-negative",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../datasets/gen-plate-dataset'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "musical-international",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "pretty-passport",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# dataset creations\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "class ClassificationDataset:\n",
    "    def __init__(self, img_paths, targets, resize = None):\n",
    "        self.img_paths = img_paths\n",
    "        self.targets = targets\n",
    "        self.resize = resize\n",
    "        self.aug = albumentations.Compose(\n",
    "            [albumentations.Normalize(always_apply=True)]\n",
    "        )\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    def __getitem__(self, item_index):\n",
    "        img = Image.open(self.img_paths[item_index])\n",
    "        targets = self.targets[item_index]\n",
    "        \n",
    "        if self.resize is not None:\n",
    "            img = img.resize((self.resize[0], self.resize[1]), resample= Image.BILINEAR)\n",
    "        \n",
    "        img = np.array(img)\n",
    "        augmented = self.aug(image = img)\n",
    "        img = augmented['image']\n",
    "        img = np.transpose(img, (2, 1, 0)).astype(np.float32)\n",
    "        return {\n",
    "            'imgs': torch.tensor(img, dtype=torch.float),\n",
    "            'targets': torch.tensor(targets, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-surname",
   "metadata": {},
   "source": [
    "# Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "appropriate-spanking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# engine\n",
    "\n",
    "def train_fn(model, data_loader, optimizer):\n",
    "    model.train()\n",
    "    fin_loss = 0\n",
    "    tk = tqdm(data_loader, total=len(data_loader))\n",
    "    for data in tk:\n",
    "        for k, v in data.items():\n",
    "            data[k] = v.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        _, loss = model(**data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        fin_loss += loss.item()\n",
    "        \n",
    "    return fin_loss / len(data_loader)\n",
    "\n",
    "def eval_fn(model, data_loader, optimizer):\n",
    "    model.eval()\n",
    "    fin_loss = 0\n",
    "    fin_preds = []\n",
    "    with torch.no_grad():\n",
    "        tk = tqdm(data_loader, total=len(data_loader))\n",
    "        for data in tk:\n",
    "            for k, v in data.items():\n",
    "                data[k] = v.to(DEVICE)\n",
    "            batch_preds, loss = model(**data)\n",
    "            \n",
    "            fin_loss += loss.item()\n",
    "            fin_preds.append(batch_preds)\n",
    "        \n",
    "        fin_preds = torch.cat(fin_preds, dim=0)\n",
    "        return fin_preds, fin_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-brick",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "alternate-cornell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "class PlateModel(nn.Module):\n",
    "    def __init__(self, num_chars):\n",
    "        super(PlateModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 128, kernel_size=(3,3), padding=(1,1))\n",
    "        self.max_pool_1 = nn.MaxPool2d(kernel_size=(2,2))\n",
    "        self.conv2 = nn.Conv2d(128, 64, kernel_size=(3,3), padding=(1,1))\n",
    "        self.max_pool_2 = nn.MaxPool2d(kernel_size=(2,2))\n",
    "        \n",
    "        self.linear1 = nn.Linear(768, 512)\n",
    "        self.linear2 = nn.Linear(512, 64)\n",
    "        self.drop = nn.Dropout(0.2)   # doesn't change size\n",
    "        \n",
    "        self.gru = nn.GRU(64, 32, bidirectional=True, num_layers=2, dropout=0.25)\n",
    "        self.output = nn.Linear(64, num_chars + 1)\n",
    "        \n",
    "    def forward(self, imgs, targets=None):\n",
    "        bs, c, w, h = imgs.size()\n",
    "        # print(bs, c, w, h)    # for debugging\n",
    "        x = F.relu(self.conv1(imgs))\n",
    "        # print('Conv1', x.size())\n",
    "        x = self.max_pool_1(x)\n",
    "        # print('MaxPool', x.size())\n",
    "        x = F.relu(self.conv2(x))\n",
    "        # print('Conv2', x.size())\n",
    "        x = self.max_pool_2(x) # 1, 64, 212, 64\n",
    "        # print('MaxPool', x.size())\n",
    "        \n",
    "        # to brind width first but in our case it's properly arranged\n",
    "        # x = x.permute(0, 3, 1, 2) # 1, 75, 64, 18\n",
    "        x = x.view(bs, x.size(2), -1)\n",
    "        # print('View', x.size())\n",
    "        \n",
    "        x = self.linear1(x)\n",
    "        # print('Linear1', x.size())\n",
    "        x = self.linear2(x)\n",
    "        x = self.drop(x)\n",
    "        # print('Linear2', x.size())\n",
    "        \n",
    "        x, _ = self.gru(x)\n",
    "        # print('GRU', x.size())\n",
    "        \n",
    "        x = self.output(x)\n",
    "        # print('output', x.size())\n",
    "        \n",
    "        x = x.permute(1, 0, 2)\n",
    "        if targets is not None:\n",
    "            # CTC\n",
    "            log_softmax_values = F.log_softmax(x, 2)\n",
    "            input_lengths = torch.full(\n",
    "                size=(bs,), fill_value = log_softmax_values.size(0), dtype=torch.int32\n",
    "            )\n",
    "            # print('input lengths', input_lengths)\n",
    "            target_lengths = torch.full(\n",
    "                size=(bs,), fill_value = log_softmax_values.size(1), dtype=torch.int32\n",
    "            )\n",
    "            # print('target lengths', target_lengths)\n",
    "            loss = nn.CTCLoss(blank=0)(\n",
    "                log_softmax_values, targets, input_lengths, target_lengths\n",
    "            )\n",
    "            return x, loss\n",
    "        \n",
    "        return x, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "corresponding-bedroom",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for debugging model\n",
    "\n",
    "cm = PlateModel(19)\n",
    "img = torch.rand(5, 3, IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "targets = torch.randint(1, 6, (5, 5))\n",
    "x, loss = cm(img, targets)\n",
    "\n",
    "del(cm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-discharge",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "national-belize",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate img & target list\n",
    "\n",
    "def get_img_label():\n",
    "    '''Returns tuple of img filename list and target_label list.'''\n",
    "    img_files = glob.glob(os.path.join(DATA_DIR, '*.png'))\n",
    "    targets_orig = [x.split('/')[-1][ : -4] for x in img_files]\n",
    "    return img_files, targets_orig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "conceptual-numbers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target preprocessing\n",
    "\n",
    "def get_target_list(target_orig):\n",
    "    targets = [[c for c in x] for x in targets_orig]\n",
    "    targets_flat = [c for clist in targets for c in clist]\n",
    "    return targets, targets_flat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acoustic-hanging",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target encoding\n",
    "\n",
    "def encode_labels(targets, targets_flat):\n",
    "    lbl_enc = preprocessing.LabelEncoder()\n",
    "    lbl_enc.fit(targets_flat)\n",
    "    targets_enc = [lbl_enc.transform(x) for x in targets]\n",
    "    targets_enc = np.array(targets_enc) + 1\n",
    "    return lbl_enc, targets_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "specified-certificate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode\n",
    "\n",
    "def decode_predictions(preds, encoder, collapse_repeated=True):\n",
    "    ''' Decodes CTC String to normal string'''\n",
    "    preds = preds.permute(1, 0, 2)\n",
    "    preds = torch.softmax(preds, 2)\n",
    "    preds = torch.argmax(preds, 2)\n",
    "    preds = preds.detach().cpu().numpy() # change cpu to cuda if training on gpu\n",
    "    cap_preds = []\n",
    "    for j in range(preds.shape[0]):\n",
    "        temp = ''\n",
    "        prev_char = None\n",
    "        for k in preds[j]:\n",
    "            k = k-1\n",
    "            # k = -1 mean a empty value\n",
    "            if (k == -1) or (collapse_repeated and k==prev_char):\n",
    "                continue\n",
    "            else:\n",
    "                # print(encoder.inverse_transform([k]), k)\n",
    "                temp += encoder.inverse_transform([k])[0]\n",
    "            prev_char = k\n",
    "        tp = \"\" + temp\n",
    "        # print(preds[j], targets[j], tp)\n",
    "        cap_preds.append(tp)\n",
    "    return cap_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "nervous-sailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks if all characters of targets are in predictions\n",
    "\n",
    "def char_accuracy(preds, targets):\n",
    "    sum_accuracy = 0\n",
    "    total_preds = 0\n",
    "    for index in range(len(preds)):\n",
    "        correct_char = 0\n",
    "        for char in preds[index]:\n",
    "            if char in targets[index]:\n",
    "                correct_char += 1\n",
    "        accuracy = correct_char / len(preds[index])\n",
    "        sum_accuracy += accuracy\n",
    "        total_preds += 1\n",
    "    return sum_accuracy / total_preds\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "essential-rotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks if all predictions are same as labels\n",
    "\n",
    "def label_accuracy(preds, targets):\n",
    "    correct_labels = 0\n",
    "    for index in range(len(preds)):\n",
    "        if preds[index] == targets[index]:\n",
    "            correct_labels += 1\n",
    "    return correct_labels / len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "elegant-liabilities",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing & encoding\n",
    "\n",
    "img_files, targets_orig = get_img_label()\n",
    "\n",
    "targets, targets_flat = get_target_list(targets_orig)\n",
    "lbl_enc, targets_enc = encode_labels(targets, targets_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "expanded-shaft",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data splitting\n",
    "\n",
    "( train_imgs, test_imgs,\n",
    " train_targets,\n",
    " test_targets,\n",
    " train_orig_targets,\n",
    " test_orig_targets \n",
    ") = model_selection.train_test_split(\n",
    "    img_files, targets_enc, targets_orig, test_size=0.1, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "necessary-directive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataset & loader\n",
    "\n",
    "train_dataset = ClassificationDataset(\n",
    "                    img_paths=train_imgs, \n",
    "                    targets=train_targets,\n",
    "                    resize=(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "                )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                    train_dataset,\n",
    "                    batch_size = BATCH_SIZE,\n",
    "                    num_workers = NUM_WORKERS,\n",
    "                    shuffle = True\n",
    "                )\n",
    "\n",
    "# for debugging\n",
    "\n",
    "# npimg = train_dataset[0]['imgs'].numpy()\n",
    "# plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "particular-ideal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset & loader\n",
    "\n",
    "test_dataset = ClassificationDataset(\n",
    "                    img_paths = test_imgs,\n",
    "                    targets = test_targets,\n",
    "                    resize = (IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "                )\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                test_dataset,\n",
    "                batch_size = BATCH_SIZE,\n",
    "                num_workers = NUM_WORKERS,\n",
    "                shuffle = False\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "reflected-constant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, optimizer & schedular\n",
    "\n",
    "model = PlateModel(num_chars=len(lbl_enc.classes_))\n",
    "model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "schedular = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, factor=0.8, patience=5, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "constant-groove",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual training\n",
    "\n",
    "def run_training(model, train_loader, test_loader, optimizer, schedular, lbl_enc):\n",
    "    loss = {'train': [], 'valid': []}\n",
    "    accuracy = {'character': [], 'label': []}\n",
    "    \n",
    "    epoch_count = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = train_fn(model, train_loader, optimizer)\n",
    "        valid_preds, valid_loss = eval_fn(model, test_loader, optimizer)\n",
    "        \n",
    "        valid_cap_preds = []\n",
    "        for vp in tqdm(valid_preds, total=len(valid_preds)):\n",
    "            current_preds = decode_predictions(valid_preds, lbl_enc)\n",
    "            valid_cap_preds.extend(current_preds)\n",
    "            \n",
    "        char_acc = char_accuracy(test_orig_targets, valid_cap_preds)\n",
    "        label_acc = label_accuracy(test_orig_targets, valid_cap_preds)\n",
    "            \n",
    "        # calculate accuracy of model and log it   \n",
    "        pprint(list(zip(test_orig_targets[6:15], valid_cap_preds))[6:15])\n",
    "        print(f\"Epoch:{epoch}, train_loss:{train_loss}, valid_loss={valid_loss}\")\n",
    "        print(f\"char_accuracy:{char_acc}, label_accuracy:{label_acc}\")\n",
    "        \n",
    "        loss['train'].append(train_loss)\n",
    "        loss['valid'].append(valid_loss)\n",
    "        accuracy['character'].append(char_acc)\n",
    "        accuracy['label'].append(label_acc)\n",
    "        \n",
    "        epoch_count += 1\n",
    "        \n",
    "    return loss, accuracy, epoch_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-permit",
   "metadata": {},
   "source": [
    "# Start Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "protecting-translation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [00:22<00:00,  8.28it/s]\n",
      "100%|██████████| 21/21 [00:01<00:00, 14.82it/s]\n",
      "  2%|▏         | 29/1197 [00:00<00:17, 65.83it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-631a5b4082f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschedular\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlbl_enc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-c149a4847a70>\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(model, train_loader, test_loader, optimizer, schedular, lbl_enc)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mvalid_cap_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mvp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mcurrent_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlbl_enc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mvalid_cap_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-26f16df6d088>\u001b[0m in \u001b[0;36mdecode_predictions\u001b[0;34m(preds, encoder, collapse_repeated)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m''' Decodes CTC String to normal string'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# change cpu to cuda if training on gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss, accuracy, epoch_count = run_training(model, train_loader, test_loader, optimizer, schedular, lbl_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passing-justice",
   "metadata": {},
   "source": [
    "## ENDGAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-spare",
   "metadata": {},
   "source": [
    "### Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bridal-printer",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = os.path.join('..','train')\n",
    "\n",
    "try:\n",
    "    os.mkdir(TRAIN_DIR)\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "rocky-consolidation",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINER_DIR = os.path.join(TRAIN_DIR,'harshad')    # change this\n",
    "\n",
    "try:\n",
    "    os.mkdir(TRAINER_DIR)\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "annoying-density",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this for every training\n",
    "# or it will overwrite your previous data\n",
    "\n",
    "VER_DIR = os.path.join(TRAINER_DIR, 'text_recognition ver-1.0')\n",
    "\n",
    "try:\n",
    "    os.mkdir(VER_DIR)\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "usual-vehicle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving weights & optimizer\n",
    "\n",
    "WT_PATH = os.path.join(VER_DIR, 'weights.pth')\n",
    "torch.save(model.state_dict(), WT_PATH)\n",
    "\n",
    "OPTIM_PATH = os.path.join(VER_DIR, 'optimizer.pth')\n",
    "torch.save(model.state_dict(), OPTIM_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-vietnamese",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "allied-locking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving hyperparametes\n",
    "\n",
    "HYP_PATH = os.path.join(VER_DIR, 'hyperparam.json')\n",
    "\n",
    "hyper_dict = dict()\n",
    "hyper_dict[\"INITIALIED EPOCH\"] = EPOCHS\n",
    "hyper_dict[\"ACTUAL EPOCH\"] = epoch_count\n",
    "hyper_dict[\"MODEL\"] = str(model.parameters)\n",
    "hyper_dict[\"LOSS\"] = dict()\n",
    "hyper_dict[\"LOSS\"][\"train\"] = [float(train_loss) for train_loss in loss['train']]\n",
    "hyper_dict[\"LOSS\"][\"valid\"] = [float(valid_loss) for valid_loss in loss['valid']]\n",
    "hyper_dict[\"ACCURACY\"] = dict()\n",
    "hyper_dict[\"ACCURACY\"][\"char\"] = [float(train_loss) for train_loss in accuracy['character']]\n",
    "hyper_dict[\"ACCURACY\"][\"label\"] = [float(valid_loss) for valid_loss in accuracy['label']]\n",
    "\n",
    "\n",
    "hyp_file = open(HYP_PATH, \"w\")\n",
    "hyperparam_json = json.dump(hyper_dict, fp=hyp_file)\n",
    "hyp_file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-vertex",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}